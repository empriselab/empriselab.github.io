<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  <title>Research</title>

  
  <meta name="author" content="EmPRISE Lab (Inspired by the Allan Lab template)">
  

  

  

  <link rel="alternate" type="application/rss+xml" title="EmPRISE Lab" href="http://emprise.cs.cornell.edu/feed.xml">

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="EmPRISE Lab">
  <meta property="og:title" content="Research">
  <meta property="og:description" content="">

  

  
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://emprise.cs.cornell.edu/research/">
  <link rel="canonical" href="http://emprise.cs.cornell.edu/research/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@EmpriseLab">
  <meta name="twitter:creator" content="@EmpriseLab">

  <meta property="twitter:title" content="Research">
  <meta property="twitter:description" content="">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand navbar-brand-logo" href="http://emprise.cs.cornell.edu/"><img alt="EmPRISE Lab Logo" src="/assets/img/emprise-logo.png"/></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/people">People</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/research">Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/teaching">Teaching</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/outreach">Outreach</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/resources">Open Resources</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/press">Press</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/gallery">Gallery</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/apply">Apply</a>
          </li></ul>
  </div>

  

  

</nav>


  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="page-heading">
          <h1>Research</h1>
          

          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md " role="main">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">
      

      <h2>Robot-assisted Feeding</h2>

<p>
  Eating is an activity of daily living (ADL) and losing the ability to
  self-feed can be devastating. Robots have the potential to help with these
  tasks. Successful robotic assistive feeding depends on reliable bite
  acquisition, appropriate bite timing, and easy bite transfer in both
  individual and social dining settings. Automating bite acquisition is daunting
  as the universe of foods, cutlery, and human strategies is massive and the
  activity demands robust nonprehensile manipulation of a deformable
  hard-to-model target. Bite timing, especially in social dining settings, is a
  delicate dance of multimodal signaling (via gaze, facial expressions,
  gestures, and speech, to name a few), action, and sometimes coercion. Bite
  transfer constitutes a unique type of robot-human handover where the human
  needs to use the mouth. Through this project, we are developing algorithms and
  technologies that can address these challenges towards a robotic system that
  can autonomously feed people with upper-extremity mobility limitations in real
  homes.
</p>

<div class="col embed-responsive embed-responsive-16by9">
  <iframe
    class="embed-responsive-item"
    src="https://www.youtube.com/embed/-pOP7vbFqSk"
    allowfullscreen
  ></iframe>
</div>

<br />

<h2>Single-arm Robot Peeling using Adaptive Cutting Boards</h2>

<p>
  Peeling food is a complex task that requires cooperative manipulation of both
  the object to be peeled and the peeler itself. The goal of this project is to
  develop a pipeline for single-arm robots in conjunction with an adaptive
  cutting board that can autonomously carry out peeling on six food items;
  apples, cucumbers, carrots, potatoes, chinese okra, and squash. These items
  were chosen for their differences in shape, size, rigidity, flesh vs skin
  color, skin thickness, and surface topographies which should lead to the
  development of a robust pipeline that works on a diverse range of items.
</p>
<p>
  Currently, work is focused on perception - 3D reconstruction of food and
  segmentation of their peeled regions. Baseline implementations have been
  created for both, but 3D reconstruction suffers from long processing time and
  segmentation struggles with certain food items and is not invariant to
  different lighting conditions. The next direct step is to work on simulating
  food items in RCareWorld in order to curate a dataset of synthetic images in
  which items have been peeled. Using this dataset, a robust CNN will be trained
  to segment out peeled regions.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/1.png" class="gallery-img-item" />
</div>

<br />

<h2>Assistive Bathing</h2>

<p>
  People who are paraplegic (limited in mobility in the lower parts of the body)
  often require assistance with bathing from human caregivers. Furthermore, such
  bathing procedures involve wetting and drying specific areas of the body in a
  sequence that can be arduous for both the caregiver and the person receiving
  the bath. The goal of this project is to reduce such frustrations by enabling
  a basic robotic setup for autonomously bathing people with the help of a robot
  arm and various sensors.
</p>
<p>
  Current work is focused on detecting and correctly classifying wet and dry
  regions of the body. Trying to solve this problem with only raw RGB and depth
  camera data proves difficult, mainly because current computer vision methods
  are not tailored to distinguish wet regions of a surface from dry ones. With
  the help of thermal data, binary classification is much easier for many
  surfaces of the body. The immediate goal is to develop and train a neural
  network that can correctly classify these regions with a high accuracy using
  both the RGBD and thermal data from cameras. The eventual goal of the project
  is to use the network and the robot arm to wash and dry a human arm.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/2.png" class="gallery-img-item" />
</div>

<br />

<h2>Assistive Cutting and Feeding</h2>

<p>
  Many works in robotic feeding often assume that food is in bite sized pieces.
  In reality, this situation rarely holds, meaning caregivers need to cut food
  objects into smaller pieces before giving the plate of food to the feeding
  system. This project aims to tackle the problem of cutting in a feeding
  setting. One challenge of this project is getting the robot to learn how to
  use a household utensil to cut and acquire a piece of food. In food cutting,
  many works often fix the position of a tool (often a knife), meaning that a
  robot will be unable to cut objects if they use a different tool to cut.
  Another challenge is getting the robot to perform these two complex motion
  primitives in succession. Cutting and acquiring food objects are challenging
  tasks on their own, so performing these two motions in succession becomes much
  harder.
</p>
<p>
  So far, we have figured out how to get a robot to identify where to position
  the fork to make cuts. This was accomplished by getting the shape of a food
  from an RGB camera, converting the shape to a graph and finding points to make
  a cut. Next steps will include improving the efficiency of the algorithm and
  begin looking into how to get the robot to learn how to identify features of a
  tool to perform cutting and scooping.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/3.png" class="gallery-img-item" />
</div>

<br />

<h2>Soft Tactile Sensing for Assistive Robotic Manipulators</h2>

<p>
  Touch is a key sensory modality for humans, allowing us to physically perceive
  and interact with objects in the environment around us. This principle can be
  similarly extended to robots, as force and pressure-based sensing can allow
  robots to account for errors from non-contact based sensors and safely operate
  alongside humans in cluttered environments. While there exists a large body of
  work on tactile sensing for robotic end effectors using various methodologies
  (piezoresistive, capacitive, and optical to name a few), similar work on
  whole-body sensing, particularly with soft or deformable materials, is limited
  in its scope. In this project, we propose a fabric-based capacitive sensor
  design to provide whole-arm sensing capabilities for manipulation tasks with a
  Kinova Gen3 robotic arm with 7 degrees of freedom. In addition, we use
  deformable air bubbles under the sensor to provide an element of softness to
  the robot arm, improving user experience without sacrificing the accuracy of
  our sensor's observations.
</p>

<p>
  Work so far has been focused on development of the fabric capacitive sensors
  as well as supporting hardware and software. Our current goals are to develop
  and integrate the air bubble mechanism with the sensors, and benchmark the
  performance of this sensor modality in terms of detected contact force
  magnitude and location. In the near future, we aim to perform a study that
  performs simple physical HRI tasks with the Kinova arm and a human subject in
  order to evaluate the safety and comfort of human subjects in actions that
  require contact with the robot arm.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/4.jpg" class="gallery-img-item" />
</div>

<br />

<h2>Assistive Robot Simulator</h2>

<p>
  Caregiving robots have the potential to provide assistance to increase or
  prolong independence while reducing caregiving burden. However, building safe,
  efficient, and meaningful robotic solutions for caregiving can be challenging.
  The lack of access to individuals with mobility limitations and their
  caregivers hinders realistic design expectations. The large cost associated
  with real robot hardware development and maintenance can be prohibitive.
  Simulation platforms that can realistically model care-recipients, caregivers,
  robots, and the interactions between them in real-life caregiving can help to
  lower these barriers to entry and democratize this impactful field. Despite
  some commendable attempts, for instance, AssistiveGym, such a simulation
  environment that genuinely captures the world of robotic caregiving currently
  does not exist.
</p>

<p>
  We collaborated with occupational therapists, care-recipients, and their
  caregivers to identify essential components and real-world functional
  interaction. WIth the input from these stakeholders, we present RCareWorld, a
  human-centric simulation world for physical and social robot caregiving. We
  developed human-like avatars with a muscle actuation system and soft tissue
  covering the human body. We embedded the avatars in virtual homes with
  different levels of accessibility and availability of assistive devices for
  functional mobility. RCareWorld brings together robotic caregivers,
  CareAvatars, and CareHomes within a Unity-based simulation platform to provide
  a comprehensive simulation of each component in the caregiving scenario.With
  this simulation platform, we performed reinforcement learning experiments for
  physical caregiving and VR experiment for social caregiving. In the future, we
  plan to have support for more realistic human avatars and environments in the
  future.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/5.png" class="gallery-img-item" />
</div>

<br />

<h2>Food Pre Manipulation</h2>

<p>
  Pre-manipulation means the manipulation action before bite acquisition. It is
  a natural behavior to make bite acquisition easier when people eat. Before
  robots feed people, they can also skewer food items easier with
  pre-manipulation. In spite of its potential to make bite acquisition easier,
  pre-manipulation is less explored in previous work. The challenging part in
  this task lies in the diversity of food item and the difficulty to generalize.
</p>
<p>
  Currently, we perform a pilot study and collect a dataset of pre-manipulation
  from human demonstrators. We summarize the pre-manipulation action primitives
  as cut, scoop, push, and twirl. We plan to set up a simulation environment,
  where a robot pre-manipulates the food items, and then we can collect image
  data of the food item before and after pre-manipulation. After that, we plan
  to use a generative model to predict the image of the plate after each action.
  Using SPANet, we can predict the skewer successful rate and select tho most
  suitable action.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/6.png" class="gallery-img-item" />
</div>

<br />

<h2>Human-Robot Commensality</h2>

<p>
  Being able to eat independently with friends and family is considered one of
  the most memorable and important activities for people with mobility
  limitations. Robots can potentially help with this activity but robot-assisted
  feeding is a multi-faceted problem with challenges in bite acquisition, bite
  timing, and bite transfer. Bite timing in particular becomes uniquely
  challenging in social dining scenarios due to the possibility of interrupting
  a social human-robot group interaction during commensality.
</p>
<p>
  Our key insight is that bite timing strategies that take into account the
  delicate balance of social cues can lead to seamless interactions during
  robot-assisted feeding in a social dining scenario. We approach this problem
  by collecting a multimodal Human-Human Commensality Dataset (HHCD) containing
  30 groups of three people eating together. We use this dataset to analyze
  human-human commensality behaviors and develop bite timing prediction models
  in social dining scenarios. We also transfer these models to human-robot
  commensality scenarios. Our user studies show that prediction improves when
  our algorithm uses multimodal social signaling cues between diners to model
  bite timing.
</p>
<p>
  Future work could involve learning models in human-robot commensality settings
  rather than transferring from human-human scenarios.This could help minimize
  distribution shift between these two scenarios, but also could help model the
  preferences of individual users.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/7.png" class="gallery-img-item" />
</div>

<br />

<h2>Limb Repositioning</h2>

<p>
  Limb repositioning is a task that facilitates a variety of activities of daily
  living such as bathing, dressing and wheelchair transfer. The goal of this
  project is to be able to reposition the arm of someone who has been
  transferred to a wheelchair in such a way that their arms sit on the armrest.
  Since the robot will be actively interacting with human limbs, it is vital
  that we ensure safe and stable physical interactions between humans and robots
  while assuring gentle manipulation of human limbs during this process. We are
  currently developing a pipeline that accurately estimates limb pose and grasps
  the arm in a manner that is convenient for the care recipient and accounts for
  human joint constraints as the robot manipulates human limbs. We aim to have a
  system that we can personalize to each care recipient's needs based on each of
  their mobility limitations.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/8.png" class="gallery-img-item" />
</div>

<br />

<h2>Human in the Loop Robot Manipulation</h2>

<p>
  Robust Robot Manipulation involves the robot working in unfamiliar and
  uncontrolled settings. However, this kind of manipulation can involve robots
  making mistakes while trying to achieve the task at hand, and recovering from
  those mistakes can be difficult. Human in the Loop Robot Manipulation involves
  the availability of a human to whom you can ask questions that can help the
  robot recover from such mistakes. However, in a real setting, humans who can
  help the robot are often busy with other tasks as well, and cannot afford to
  guide the robot through everything it does. Constantly asking questions of the
  human, will significantly increase their workload, and is something we want to
  prevent. Hence we need to balance recovering from mistakes and/or uncertainty
  for the robot, with the cognitive workload of the human helping the robot.
</p>
<p>
  Currently, we are working on a study to determine the different factors that
  play into the cognitive workload, and running an Amazon Mechanical Turk to
  generate a dataset. What we aim to have is a model predicting the cognitive
  workload of the human, that is used to optimize for when we should ask a
  question of the user. We are currently framing this problem as a contextual
  bandit where the robot can take either one of its original actions, or query a
  user for help. We plan to see whether a model to optimize the workload of the
  user, and the uncertainty of the robot to decide when to ask questions,
  actually helps reduce the workload of the human answering questions.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/9.png" class="gallery-img-item" />
</div>

<br />

<h2>
  SPARCS - Structuring Physically Assistive Robotics with
  Stakeholders-in-the-loop
</h2>

<p>
  Existing work in physically assistive robotics for caregiving is limited in
  its ability to provide meaningful in-situ long-term assistance to
  care-recipients for the entire duration of an ADL. Lack of a common framework
  is one of the primary reasons that robotic caregiving research has evolved in
  a scattered manner, which contributes to a majority of research in this domain
  being still restricted to lab environments.
</p>
<p>
  We introduce Structuring Physically Assistive Robotics for Caregiving with
  Stakeholders-in-the-loop, or SPARCS, a framework that defines and
  systematically structures the field of robot-assisted physical caregiving. At
  the core of SPARCS, lies the four building blocks of robotic physical
  assistance -- the user, i.e., the care recipient, their environment, their
  human caregiver, and the robot caregiver. We build upon these building blocks
  and design Structured Workflows, an abstraction language common to roboticists
  and other stakeholders. We demonstrate the benefits of SPARCS for roboticists
  by utilizing them for robot-assisted feeding towards building more
  personalized systems and improving assistance. Additionally, we facilitate
  this conversation between roboticists and other stakeholders by introducing
  SPARCS-box, a web-based platform. We release this platform initially with
  Structured Workflows for the six avatars, validated by the occupational
  therapists we interviewed. In future, we plan to work on the formalism of
  SPARCS to more deeply incorporate it into the building and deployment process
  of robotic caregiving. We also intend to improve the accessibility of
  SPARCS-box to make it more resourceful for discussions and generating
  crowdsourced workflows for robotic caregiving.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/10.png" class="gallery-img-item" />
</div>

<br />

<h2>Whole-arm Manipulation using Tactile Sensing</h2>

<p>
  It is fairly common to have contacts on regions besides the hand and wrists
  when providing assistance with activities of daily living (ADLs). If we are to
  build robotic arms that provide physical assistance for these ADLs, widespread
  contact across the robot is inevitable. It is important to reason about
  contact forces when in contact with a person to ensure their safety and
  comfort. While several recent works on robot manipulation look into tactile
  sensing on the robot gripper, whole-arm tactile sensing is relatively
  underexplored. Our work proposes using an artificial whole-arm tactile sensing
  skin along with an variable impedance based MPC control scheme to perform
  physical assistance tasks in the presence of extensive contact regions.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/11.png" class="gallery-img-item" />
</div>

<br />

<h2>
  Inside-Mouth Robot-Assisted Bite Transfer for Care Recipients with Severe
  Mobility Limitations
</h2>

<p>
  Feeding a bite-sized food item acquired with an utensil to a care recipient
  having severe mobility limitations requires moving it inside their mouth to an
  appropriate position. To help with this, care recipients oftentimes use their
  tongue to guide the utensil to their preferred position for transfer of the
  bite. This activity requires perceiving the mouth of the care recipient,
  moving the food item inside it, and constantly reasoning about the contacts
  between the utensil and the care recipient's mouth to discern accidental
  collisions from intentional interactions and reacting accordingly. We plan to
  develop a robot-assisted feeding system that can autonomously feed
  care-recipients taking into account the aforementioned task considerations.
</p>

<div class="center gallery-img-container">
  <img src="/assets/img/research/12.png" class="gallery-img-item" />
</div>

<br />

<!-- <h2>Compliant Manipulation with Whole-arm Sensing</h2>

<p>
  Physical interactions between robots and humans are inevitable and desired
  during caregiving. Tactile sensing can enable a robot to infer properties of
  its surroundings from planned and incidental contact during manipulation in
  unstructured human environments. Through this project, we are developing
  solutions that can efficiently combine multimodal sensing and perception to
  develop intelligent planning and control policies for safe and efficient
  manipulation with and around humans.
</p>

<div class="col embed-responsive embed-responsive-16by9">
  <iframe
    class="embed-responsive-item"
    src="https://www.youtube.com/embed/trhqnOZ6bjU"
    allowfullscreen
  ></iframe>
</div>

<br /> -->

<h2>Featured Videos</h2>

<div class="row row-cols-3">
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/DirKkmB8mW8"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/8fJRVdOlZt0"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/qKMDVInq1gM"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/CJ66x7JfqG0"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/GEIzh5kyAoY"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/_hWjzucGkqQ"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/LRI5tAhuu3s"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/dwzvcamvXMo"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/U29P72JOess"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/AIRXa-pIV4M"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/-f2FUCvgeIA"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/MVYVSOKF5Yg"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/oTDrI0n_ruQ"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/ntATgTLoNdM"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
  <div class="col my-3">
    <div class="embed-responsive embed-responsive-16by9">
      <iframe
        class="embed-responsive-item"
        src="https://www.youtube.com/embed/-Zv1rPXxq1c"
        allowfullscreen
      ></iframe>
    </div>
  </div>
  
</div>


      

      

    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:empriselab21@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/empriselab" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/EmpriseLab" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://www.youtube.com/channel/UC964lfWIMojN48cA6FK2Fhg?view_as=subscriber" title="YouTube">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-youtube fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">YouTube</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        EmPRISE Lab (Inspired by the <a href="http://www.allanlab.org/aboutwebsite" target="_blank">Allan Lab</a> template)
        &nbsp;&bull;&nbsp;
      
      2022

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://emprise.cs.cornell.edu/">emprise.cs.cornell.edu</a>
        </span>
      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
